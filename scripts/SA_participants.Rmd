---
title: "Sentiment Analysis with R"
subtitle: "Methods Fair @UoM"
author: "Dr Giulia Grisot"
output: html_document
date: "May 15, 2025"
---

```{r}
# set options (no warning messages, no scientific notation, default TRUE for package dependencies)
options(warn=-1, scipen=999, repos = c(CRAN = "https://cran.rstudio.com"),  dependencies = TRUE)
```

Ok, now we are ready to start with the SA workshop!

We will mainly use the package 'tidytext': this contains several functions to perform sentiment analysis on text data. The package is based on the tidyverse packages (dplyr, tidyr, ggplot2, etc.), which are a collection of packages for data manipulation and visualization in R. The tidytext package provides a set of functions to manipulate and analyse text data. It provides functions to convert text data into a tidy format, and it provides functions to analyse text data using the tidyverse. The package also provides a set of functions to perform sentiment analysis on text data.

# Sentiment Analysis in R

## Prerequisites

- Basic understanding of R and RStudio
- RStudio installed on your computer
- Required R packages installed: see below in chunk

```{r, warning=FALSE, message=FALSE}
# install required packages (uncomment if you need to install the packages, and then comment again after installation is complete)

# install.packages("tidyverse")
# install.packages("ggplot2")
# install.packages("readtext")
# install.packages("textdata")
# install.packages("tidytext")
# install.packages("ggthemes")
# install.packages("wordcloud")
# install.packages("leaflet")
# install.packages("tmap")
# install.packages("tmaptools")
# install.packages("sf")
# install.packages("viridis")
# install.packages("table1")

```

### Load the necessary packages

```{r, message = FALSE, warning = FALSE, echo = FALSE}
# Load the necessary packages

library(tidyverse)
library(ggplot2)
library(readtext)
library(textdata)
library(tidytext)
library(ggthemes)
library(wordcloud)
library(leaflet)
library(tmap)
library(sf)
library(tmaptools)

```

# Introduction

Sentiment Analysis is a subfield of Natural Language Processing (NLP) - a branch of Artificial Intelligence (AI) that deals with the interaction between computers and humans using natural language. Sentiment Analysis is the process of determining the emotional 'tone' behind words. It's used to gain an understanding of the attitudes, opinions, and emotions expressed within a text.

In general, there are two ways of considering emotions: the emotions evoked by the text, and the emotions expressed by the text. Sentiment analysis is concerned with the second, with the examination of the sentiment (positive, negative, neutral) or the emotional content (joy, anger, sadness, fear etc.) embedded in the text.

In the context of Digital Humanities, sentiment analysis can be used to understand human behaviour in digital spaces, analyse texts from books, social media, blogs, websites, and other digital platforms to extract useful insights.

In this workshop, we will have a look at how sentiment analysis can be performed with R. This is a script thought for absolute beginners, so the scripts are written for you to understand the process and the logic behind the code. You just need to click on the 'Run' button to execute the code in the code chunks. Obviously, if you are already familiar with R, you can skip the explanations and just run the code, or edit it as you prefer.

## Tidytext polarity - Twitter data

We will start by using some tweets, which we downloaded from the Keggle (website)[https://www.kaggle.com/code/monogenea/quick-guide-to-game-of-thrones-s8-twitter].

In our directory, we have a small version of the original dataset, a collection of tweets from the Game of Thrones season 8. We will use this dataset to calculate the polarity of the tweets.

Let's start by loading the tweets from the dataset.

```{r}

# In this chunk you can see how I imported and reduced the dataset (you can download the original dataset from the link above and uncomment. the following to repeat the operation with your desired number of tweets)

# we can read the rds file we created
gotTwitter_sample <- readRDS("data/gotTwitter_sample.rds")

```

Let's have a look at the structure of the dataset.

```{r}
# if you want you can have a look at the structure of the dataset
# str(gotTwitter_sample)
```

```{r}
# or have a look just at the first few rows of the dataset
head(gotTwitter_sample, 10)
```

You can see that there are a lot of variables, and for this exercise we will only use the 'text', 'user_id' variables, and retain information about the time and place of the tweet. We also want to create a unique id for each tweet, called 'tweet_id'.
You might have noticed that the user_id has a variable and not very informative name. We can use the user_id to create a new variable that is more informative, for example by adding a prefix to the user_id. We can use the paste0() function to create the new variable, and the mutate() function to add it to the dataset.

```{r}

# Convert time format from UTC to EDT, select only the variables we need, add a unique id for each tweet, and rename the user_id variable

user_ids <- gotTwitter_sample %>%
  dplyr::select(user_id) %>%
  distinct() %>%
  mutate(user_id_new = paste0("user_", row_number()))

# Here we are using the dplyr package to manipulate the data. We are using the mutate() function to create a new variable, and the paste0() function to create the new variable. We are also using the select() function to select the variables we need, and the distinct() function to remove duplicates.

gotTwitter_sample <- gotTwitter_sample %<>%
  dplyr::mutate(created_at = as_datetime(created_at, tz = "UTC")) %>% 
  dplyr::mutate(created_at = with_tz(created_at, tzone = "America/New_York")) %>%
  dplyr::select(text, user_id, created_at, geo_coords, place_full_name) %>%
  dplyr::mutate(tweet_id = row_number()) %>%
  left_join(user_ids, by = "user_id") %>%
  mutate(user_id = user_id_new) %>%
  select(-user_id_new)

# we can also remove the user_ids object from the environment
remove(user_ids)

```

For this exercise, we will use the dictionary 'nrc', included in the tidytext package. The NRC lexicon is a sentiment lexicon that contains a list of words and their associated emotions. The lexicon was created by the National Research Council of Canada, and it contains a list of words and their associated emotions. The lexicon is used to calculate the sentiment of a text, and it is used to calculate the sentiment of a text.

```{r}
# load and observe the polarity dictionaries included in the tidytext

nrc_lex <- tidytext::get_sentiments("nrc")
bing_lex <- tidytext::get_sentiments("bing")
afinn_lex <- tidytext::get_sentiments("afinn")

# we cans also use the jockers lexicon, which is included in the qdapDictionaries package. The lexicon was created by the National Research Council of Canada, and it contains a list of words and their associated emotions. The lexicon is used to calculate the sentiment of a text, and it is used to calculate the sentiment of a text.
jockers_lex <- lexicon::hash_sentiment_jockers %>%
  rename(word = 1, jockers_polarity = 2)

# you can also try to see the end of the dictionary, uncomment the following and execute the chunk
# tail(hash_sentiment_huliu)

```

You can see that these all contain a list of words and their sentiment (or polarity/valence) scores. The scores generally have a numerical range, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. Sometimes they have discrete values, such as 'positive' and 'negative', and/or a list of emotions ('joy', 'anger', 'sadness', etc.), with or without associated sentiment scores.

The bing lexicon contains a list of positive and negative words (6786), along with their sentiment scores. The sentiment scores are either positive or negative.

The afinn lexicon contains a list of words (2477), along with their sentiment scores. The sentiment scores range from -5 to 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

Nrc lexicon contains a list of words (13914), along with their sentiment scores. The sentiment scores are either positive or negative. The lexicon also contains a list of emotions (8) and their sentiment scores.

The jockers lexicon contains a list of words (2477), along with their sentiment scores. The sentiment scores range from -5 to 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

Now that we have the tweets and the polarity dictionaries, we can calculate the polarity of the tweets. 

```{r}
# look at the column names of our dataset
colnames(gotTwitter_sample)
```

The user_id will be used to group the text (it could be that the same user writes multiple tweets), and the text is the text of the document. We can use these two columns to calculate the polarity of the tweets.

First, we need to split the texts into tokens, so that we can map the sentiment lexicon to the tokens. We can use the unnest_tokens() function from the tidytext package to split the text into tokens. The unnest_tokens() function takes a data frame and a column name as input, and it returns a data frame with the tokens in the specified column.

```{r}

gotTwitter_tokens <- gotTwitter_sample %>%
  unnest_tokens(input = text,
                output = token) %>%
  mutate(token_id = row_number())

```


Now we can match the tokens to the sentiment lexicon using the left_join() function from the dplyr package. The left_join() function takes two data frames as input, and it returns a data frame with the matching rows from both data frames. Let's use the NRC lexicon to match the tokens to the sentiment lexicon.

```{r}
# Join the sentiment lexicon to the text data

gotTwitter_tokens_NRC <- gotTwitter_tokens %>%
  left_join(nrc_lex,
            by = c("token"="word"))

```


```{r}

gotTwitter_tokens_NRC %>%
  count(user_id, index = tweet_id %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```


Let's plot this

```{r}
#  Create a barplot of the sentiment scores using ggplot2

gotTwitter_tokens_NRC %>%
  filter(!is.na(sentiment)) %>%
  group_by(sentiment) %>%
  count() %>%
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) +
  geom_col(stat = "identity") +
  labs(title = "Sentiment scores of tweets (nrc lexicon)",
       x = "User",
       y = "Total sentiment score")

```

Let's try with a different lexicon that has continuous values, such as the jockers lexicon. The jockers lexicon has a different scale than the NRC lexicon, so the results will be different.

```{r}

# Join the sentiment lexicon to the text data

gotTwitter_tokens_jockers <- gotTwitter_tokens %>%
  left_join(jockers_lex,
            by = c("token"="word"))
```

In this case, rather than a barplot, we can use a histogram to visualize the sentiment scores of the tweets. We can use the ggplot2 package to create the histogram, and the scale_fill_gradient2() function to specify the color of the plot.

```{r}

# Create a histogram of the sentiment scores using ggplot2

gotTwitter_tokens_jockers %>%
  ggplot(aes(x = jockers_polarity)) +
  geom_histogram(binwidth = 0.2, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  scale_color_viridis_c() +
  labs(title = "Sentiment scores of tweets (jockers lexicon)",
       x = "User",
       y = "Total sentiment score")

```

We can also summarise the data by 'negative' and 'positive' sentiment, and pront the average values, median and standard deviation.

```{r}

# Create a summary table of the sentiment scores using dplyr

gotTwitter_tokens_jockers %>%
  mutate(sentiment = ifelse(jockers_polarity < 0, "negative", "positive")) %>%
  group_by(sentiment) %>%
  summarise(mean = mean(jockers_polarity, na.rm = T),
            median = median(jockers_polarity, na.rm = T),
            sd = sd(jockers_polarity, na.rm = T)) %>%
  arrange(desc(mean)) %>%
  head(10)

```

We can also create a wordcloud to visualize the most frequent words in the text, and their sentiment.

```{r}
# first we need to make a list of the positive and negative words in the dataset, and calculate their frequency

positive_words <- gotTwitter_tokens_jockers %>%
  filter(jockers_polarity > 0) %>%
  group_by(token) %>%
  summarise(Freq = n()) %>%
  ungroup() %>%
  arrange(desc(Freq)) %>%
  as_tibble() %>%
  dplyr::rename(word = token)

negative_words <- gotTwitter_tokens_jockers %>%
  filter(jockers_polarity < 0) %>%
  group_by(token) %>%
  summarise(Freq = n()) %>%
  ungroup() %>%
  arrange(desc(Freq)) %>%
  as_tibble() %>%
  dplyr::rename(word = token)


wordcloud(positive_words$word, positive_words$Freq, max.words = 100, colors = "#9b2226") 
wordcloud(negative_words$word, negative_words$Freq, max.words = 100, colors = "darkgreen")

```


```{r}
library(reshape2)

gotTwitter_tokens_jockers %>%
  filter(!is.na(jockers_polarity)) %>%
  mutate(sentiment = ifelse(jockers_polarity < 0, "negative", "positive")) %>%
  count(token, sentiment, sort = TRUE) %>%
  acast(token ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkgreen", "#9b2226"),
                   max.words = 100,
                   )
```

Perfect! Now we have a good understanding of how to compute the polarity of the texts in our corpus of tweets with qdap. We can also use the polarity scores to create visualizations of the sentiment of the tweets, and to summarize the sentiment of the tweets.

## EXCERCISE 1

Can you think of other ways to visualize the polarity of the tweets in our dataset?
For example, you could use the location to create a map of the polarity scores, or you could use the time to create a time series of the polarity scores.

Try and find another way to visualize the polarity of the tweets in our dataset, and share your code with the group.

```{r}
# your turn!

```


## Maps of sentiment

With the corpus of tweets we have, we can also create a map of the sentiment of the tweets. We can use the latitude and longitude of the tweets to create a map of the sentiment of the tweets. We can use the leaflet and tmap packages to create the map, and the ggplot2 package to create the sentiment scores. 


We can create a map of the sentiment of the tweets. We can use the leaflet package to create the map, and the ggplot2 package to create the sentiment scores.

```{r}

gotTwitter_tokens_jockers[c('latitude', 'longitude')] <- str_split_fixed(gotTwitter_tokens_jockers$geo_coords, ' ', 2)

gotTwitter_tokens_jockers <- gotTwitter_tokens_jockers %>%
  mutate(latitude = as.numeric(latitude),
         longitude = as.numeric(longitude))

# We can aggregate the value per tweet_id and user_id, and then we can use the mean value to create a map of the sentiment of the tweets. We can use the leaflet package to create the map, and the ggplot2 package to create the sentiment scores.

gotTwitter_sample_lat_lng <- gotTwitter_tokens_jockers %>%
  group_by(tweet_id, user_id, latitude, longitude) %>%
  summarise(jockers_polarity = mean(jockers_polarity, na.rm = T)) %>%
  filter(!is.na(jockers_polarity)) %>%
  ungroup() %>%
  left_join(gotTwitter_sample %>%
              select(tweet_id, text), by = "tweet_id")


# Plot map of where the tweets are coming from with the tmap package
library(tmap)
library(tmaptools)
library(sf)
library(leaflet)

# Create a simple map

# first we need to create an object compatible with the tmap package that comprises the latitudes and longitudes of the tweets. We need to use the set_as_sf() function to convert the dataset to a spatial object, and the st_as_sf() function to convert the dataset to a spatial object. We also need to specify the coordinates of the dataset using the coords argument, and the coordinate reference system (CRS) using the crs argument. In this case, we will use the WGS 84 coordinate reference system, which is the most commonly used coordinate reference system for geographic data.
# the set_as_sf() function is part of the tmaptools package, and the st_as_sf() function is part of the sf package.

DT_sf = st_as_sf(gotTwitter_sample_lat_lng, coords = c("longitude", "latitude"), crs = 4326)

# now we can create a simple map using the tm_shape() function from the tmap package, and the tm_dots() function to add the dots to the map. We can also use the tm_text() function to add the text to the map, and the tm_facets() function to create a faceted map.

# we can color the dots based on the polarity score of the tweets, and we can use the tm_fill() function to specify the color of the dots. We can also use the tm_borders() function to add borders to the dots, and the tm_layout() function to specify the layout of the map.

tmap_mode("view")

tm_shape(DT_sf) +
  tm_dots(fill = "jockers_polarity",
          fill.legend = tm_legend(
            "Jocker's polarity",
            orientation = "landscape",
            ),
          fill.scale = tm_scale_continuous(midpoint = NA),
          # size = "jockers_polarity"
          size = 0.4,
          hover = T
  )

```

Lovely!

## EXCERCISE 2

How would you go about visualising the sentiment over time? Can you create a time series of the polarity scores of the tweets in our dataset?

```{r}
# your turn!


```


Well done!

You can always re-run all the code in this document, so because we are about to run a different type of sentiment analysis on a different corpus, you can now remove all the items from the environment pane to avoid confusion, and free the cache.

If anything does not work, just reload them libraries/packages at the beginning of the document.

```{r}
# remove all items from the environment pane
rm(list = ls())
gc() 
```

Let's move on!


# Corpus of novels

In this exercise, you will use the tidytext package to perform sentiment analysis on a set of texts. You will use the get_sentiments() function to load a sentiment lexicon, and you will use the left_join() function to join the sentiment lexicon to the text data. You will then use the count() function to count the number of positive and negative words in each text, and you will use the ggplot2 package to visualize the results.

The corpus we will use is a small collection of fictional texts, specifically the novels by Jane Austen. There is a package called 'janeaustenr' that contains the texts of the novels by Jane Austen. The package contains a collection of texts, and it is used to perform text analysis on the texts. The package contains a collection of texts, and it is used to perform text analysis on the texts.

Let's create our corpus:

```{r}
# Load the janeaustenr package
library(janeaustenr)

# Load the text data
text_df <- austen_books() %>%
  group_by(book) %>%
  rename(doc_id = book) %>%
  summarise(text = paste(text, collapse = "\n"))

# If we had the texts in a different format, we could use the readtext package to load the texts. Assuming the txt files are in a folder called 'corpus', we could use the readtext() function to load the texts. The readtext() function takes a folder name as input, and it returns a data frame with the text data. The readtext() function also takes a pattern argument, which is used to specify the pattern of the text files to be read. The pattern argument is used to specify the pattern of the text files to be read.

# text_df <- readtext("corpus") %>%
#   mutate(doc_id = row_number()) %>%
#   select(doc_id, text)

```

Let's start again by loading the sentiment lexicon from the tidytext package.

```{r}
# Load the sentiment lexicons
bing_lex <- get_sentiments("bing")
nrc_lex <- get_sentiments("nrc")
afinn_lex <- get_sentiments("afinn")

# we can also loadand save the jockers lexicon we used previously
jockers_lex <- lexicon::hash_sentiment_jockers %>%
  rename(word = 1, jockers_polarity = 2)

# did you see them appear in the environment pane?
# click on them to see the structure of the lexicons

```

Let's have a look at the content of the object.

```{r}
# head(text_df)

```

You can see that the corpus is made of two columns, 'doc_id' and 'text'. The 'doc_id' is a unique identifier for each text, and the 'text' is the text of the document. The doc_id also contains metadata about the texts, such as the author, the title, and the date of publication, which we can use to group the texts.

What we normally want to do at this point is to define which 'unit' are we going to use to calculate the sentiment. We might be interested in sentences or paragraphs for example, rather than the whole document. We can use the unnest_tokens() function from the tidytext package to tokenize the text data, and we can use the left_join() function to join the sentiment lexicon to the text data. (you could also use inner_join, which would only retain the words that are in the lexicon).

```{r}
#  we could have a corpus divided into paragraphs

text_paragraphs <- text_df %>%
  unnest_tokens(paragraph, text, token = "paragraphs", to_lower = F) %>%
  group_by(doc_id) %>%
  mutate(paragraph_id = seq_along(paragraph)) %>%
  ungroup()

```


```{r}
# or one divided into sentences

text_sentences <- text_paragraphs %>%
  unnest_sentences(input = paragraph,
                   output = sentence,
                   to_lower = F) %>%
  group_by(doc_id) %>%
  mutate(sentence_id = seq_along(sentence)) %>%
  ungroup()

```


If you have a look at the paragraph dataset, you'll notice however that in our case this is not ideal: different texts have different formats, and there is no consistent way to split the texts into paragraphs. We will use the sentences instead.

```{r}

# we don't need anymore the original text_df
remove(text_df)
# we can also remove the text_paragraphs object from the environment
remove(text_paragraphs)

```

Ok, so now we can join the sentiment lexicon to the text data using the left_join() function.

```{r}
# Join the sentiment lexicon to the text data
text_sentiment_bing <- text_sentences %>% # this is to our dataset
  unnest_tokens(input = sentence, 
                output = token) %>% # this is to tokenize the text data
  left_join(bing_lex,
             by = c("token"="word")) %>%
  filter(!is.na(sentiment))

```

Note we will get a warning telling us that an 'unexpected many-to-many' join has been performed. This is because some words in the lexicon have both positive and negative sentiment, and the join will create multiple rows for each word.

Fisrt, we can use the count() function to count the number of positive and negative words in each text, and we can use the ggplot2 package to visualize the results. We still have the column doc_id to group the texts, and we can use the reorder() function to order the texts by their sentiment score.

```{r}

# Create a barplot of the sentiment scores using ggplot2
library(ggplot2)

text_sentiment_bing %>% # this is to our dataset
  count(doc_id, sentiment) %>% # this is to count the number of positive and negative words in each text
  spread(sentiment, n, fill = 0) %>% # this is to spread the sentiment scores to separate columns
  mutate(sentiment = positive - negative) %>% # this is to calculate the sentiment score for each text
  ggplot(aes(x = reorder(doc_id, sentiment),
             y = sentiment,
             fill = sentiment)) + # this is to create a barplot of the sentiment scores
  geom_bar(stat = "identity") +
  scale_fill_viridis_c() +
  theme_minimal() + 
  # let's flip coordinates and remove the legend
  coord_flip() +
  theme(
    # axis.text.x = element_text(angle=60, hjust=1),
        legend.position = "none") +
  labs(title = "Sentiment scores of texts (bing lexicon)",
       x = "Text",
       y = "Total sentiment score")

```

Pretty neat, right? We calculated the sentiment score as the diffrence between the number of positive and negative words in each text. We can see that the sentiment scores of the texts are predominantly negative, with a few positive texts. Joseph Conrad's Nostromo is the most negative text, while Charlotte Bronte's Villette is the most positive text

## EXCERCISE 3

Can you repeat this using a different lexicon?

```{r}

text_sentiment_jockers <- text_sentences %>%
  unnest_tokens(input = sentence, 
                output = token) %>%
  left_join(jockers_lex,
             by = c("token"="word"))
```

Lets plot the results. Remember that jocker's lexicon has a different scale than bing's, so the results will be different.

```{r}

# Create a barplot of the sentiment scores using ggplot2. Note that the jockers lexicon has continuous sentiment scores, so we can calculate the sentiment score for each text as the difference between the number of positive and negative words in each text.

text_sentiment_jockers %>%
  group_by(doc_id) %>%
  summarise(jockers_polarity = mean(jockers_polarity, na.rm = T)) %>% # we want to calculate the sentiment score for each text
  ggplot(aes(x = reorder(doc_id, jockers_polarity),
             y = jockers_polarity,
             fill = jockers_polarity)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_c() +
  theme_minimal() +
  coord_flip() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
        legend.position = "none") +
  labs(title = "Mean polarity scores of texts (jockers lexicon)",
       x = "Text",
       y = "Mean polarity score")



```



```{r}
# your turn!
text_sentiment_nrc <- text_sentences %>%
  unnest_tokens(input = sentence, 
                output = token) %>%
  left_join(nrc_lex,
             by = c("token"="word"))
```


With nrc we can also look at the emotions detected in the texts.

```{r}

# Create a barplot of the discrete emotions scores using ggplot2

text_sentiment_nrc %>%
  filter(sentiment != "positive" & sentiment != "negative") %>% # we want ton look only at the emotions
  count(doc_id, sentiment) %>%
  ggplot(aes(x = doc_id,
             y = n,
             fill = sentiment)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_d() +
  theme_minimal() +
  coord_flip() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
        legend.position = "left") +
  labs(title = "Emotion scores of texts (nrc lexicon)",
       x = "Text",
       y = "Total emotion score")

```

## Sentiment scores of tokens

Often we might be interested in exploring the sentiment more closely, rather than only looking at the values, or at the sentiment of an entire document. 

Like we did before, we can use the unnest_tokens() function from the tidytext package to tokenize the text data, and we can use the left_join() function to join the sentiment lexicon to the tokenized text data. 

This time we can use the afinn lexicon to calculate the sentiment of each sentence in the text data. The afinn lexicon contains a list of words (2477), along with their sentiment scores. The sentiment scores range from -5 to 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}

# Let's create a tokenized version of the text data, and we join the sentiment lexicon to the tokenized text data using the left_join() function.

text_sentiment_afinn <- text_sentences %>%
  unnest_tokens(input = sentence, 
                output = token, drop = F) %>% # we want to retain the sentence column
  left_join(afinn_lex,
             by = c("token"="word")) 

```

Now that we have a list of the sentiment tokens found in our corpus, we can for instance produce a table that summarises the most frequent sentiment words in the text, and their sentiment. We can use the count() function to count the number of positive and negative words in the text, and the ggplot2 package to visualize the results.

```{r}

# Create a frequency table of the sentiment words

text_sentiment_afinn %>%
  filter(!is.na(value)) %>%
  mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
  count(token, polarity) %>%
  group_by(polarity) %>%
  top_n(15, n) %>%
  ungroup()

```

Or we can count them by novel.

```{r}

# Create a frequency table of the sentiment words by novel

text_sentiment_afinn %>%
  filter(!is.na(value)) %>%
  mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
  count(doc_id, token, polarity) %>%
  group_by(doc_id, polarity) %>%
  top_n(15, n) %>%
  ungroup()

```

Often it is easier to visualize the results. We can use a barplot to visualize the most frequent words in the text, and their sentiment. We can use the ggplot2 package to create the frequency plot, and the scale_fill_gradient2() function to specify the color of the plot.

```{r}

# Create a frequency plot of the sentiment words using ggplot2

text_sentiment_afinn %>%
  filter(!is.na(value)) %>%
  mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
  count(token, polarity) %>%
  group_by(polarity) %>%
  top_n(15, n) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(token, n),
             y = n,
             fill = polarity)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
    legend.position = "none",
    axis.title.x = element_blank()) +
  coord_flip() +
  labs(title = "Frequency of sentiment words in the text data",
       x = "Word",
       y = "Frequency") +
  facet_wrap(~polarity, scales = "free_y")

```

## EXCERCISE 4

Can you repeat the above steps to calculate the most frequent sentiment words in each novel in our corpus?

```{r, solution, echo = FALSE}

# Possible solution:


text_sentiment_afinn %>%
  filter(!is.na(value)) %>%
  mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
  count(token, polarity, doc_id) %>%
  group_by(polarity) %>%
  top_n(30, n) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(token, n),
             y = n,
             fill = polarity)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
    strip.text = element_text(angle=60, hjust=.5),
    legend.position = "none",
    # axis.title.x = element_blank()
    ) +
  coord_flip() +
  labs(title = "Frequency of sentiment words in the text data",
       x = "Word",
       y = "Frequency") +
  facet_grid(polarity~doc_id, scales = "free_y")

```

```{r}

# Another solution:

# positive words only 

text_sentiment_afinn %>%
  anti_join(tibble(token = stopwords::stopwords('en'))) %>%
  filter(value > 0) %>%
  # summarize count per word per book
  count(doc_id, token) %>%
  # get top 15 words per book
  group_by(doc_id) %>%
  slice_max(order_by = n, n = 15) %>%
  mutate(word = reorder_within(token, n, doc_id)) %>%
  # create barplot
  ggplot(aes(x = reorder(token, n), y = n, fill = doc_id)) +
  geom_col() +
  # scale_x_reordered() +
  labs(
    title = "Top 15 positive words per book",
    x = NULL,
    y = "Word count"
  ) +
  facet_wrap(facets = vars(doc_id), scales = "free") +
  coord_flip() +
  theme(legend.position = "none") +
  scale_fill_viridis_d()


# and negative words only

text_sentiment_afinn %>%
  anti_join(tibble(token = stopwords::stopwords('en'))) %>%
  filter(value < 0) %>%
  # summarize count per word per book
  count(doc_id, token) %>%
  # get top 15 words per book
  group_by(doc_id) %>%
  slice_max(order_by = n, n = 15) %>%
  mutate(word = reorder_within(token, n, doc_id)) %>%
  # create barplot
  ggplot(aes(x = reorder(token, n), y = n, fill = doc_id)) +
  geom_col() +
  # scale_x_reordered() +
  labs(
    title = "Top 15 negative words per book",
    x = NULL,
    y = "Word count"
  ) +
  facet_wrap(facets = vars(doc_id), scales = "free") +
  coord_flip() +
  theme(legend.position = "none") +
  scale_fill_viridis_d(option = "magma")


```


Nice!

## Sentiment scores of sentences

If, instead, we are interested in sentences, we can aggregate values for each sentence, and make a new colum to see to tokens with positive and negative sentiment.

We can sum the sentiment scores for each sentence, and we can also retain all the tokens that have an associated sentiment score. We can use the aggregate() function to sum the sentiment scores for each sentence, and the spread() function to spread the sentiment scores to separate columns. We can also use the rename() function to rename the columns, and the left_join() function to join the sentiment lexicon to the tokenized text data.

```{r}

text_sentiment_afinn_sentence <- 
  left_join(
    # we want to calculate the sentiment score for each sentence
    text_sentiment_afinn %>%
      mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
      aggregate(value ~ doc_id + sentence_id + sentence + polarity, data = ., sum) %>%
      spread(polarity, value, drop = T) %>%
      # rename the columns
      rename(positive_value = positive,
             negative_value = negative),
    
    # we want to retain all the tokens that have a sentiment score, so we can use the left_join() function to join the sentiment lexicon to the tokenized text data
    text_sentiment_afinn %>%
      mutate(polarity = ifelse(value < 0, "negative", "positive")) %>% # we want to calculate the sentiment score for each sentence
      aggregate(token ~ doc_id + sentence_id + sentence + polarity, data = ., paste, collapse = ", ") %>%
      spread(polarity, token, drop = T) %>%
      # rename the columns
      rename(sent_words_pos = positive,
             sent_words_neg = negative) %>%
      # let's make sure all the NULL and NA are not character strings but actual 'NA'
      mutate(sent_words_neg = ifelse(sent_words_neg == "NA", NA, sent_words_neg),
             sent_words_neg = ifelse(sent_words_neg == "NULL", NA, sent_words_neg),
             sent_words_pos = ifelse(sent_words_pos == "NA", NA, sent_words_pos),
             sent_words_pos = ifelse(sent_words_pos == "NULL", NA, sent_words_pos))
  )

```

Let's have a look at the structure of the new dataset.

```{r}
head(text_sentiment_afinn_sentence)

```

Remember we can also use again qdap polarity() and have a look at the words that were used to calculate the sentiment of each sentence. We don't need to do it now, but you can try this at home.

```{r}
# Calculate the sentiment of each sentence in the text data using qdap polarity()

# sentence_polarity_qdap <- text_sentences %$% polarity(
#   text.var = sentence,
#   grouping.var = sentence_id
# )
# 
# view(sentence_polarity[["all"]])

```

## EXCERCISE 5

Can you find out which sentences have the highest and lowest sentiment scores in our corpus, for each novel?

```{r}
# We can use the arrange() function to order the sentiment scores of the sentences in each text, and the head() function to select the top and bottom sentences.

# Find out what is the sentence with highest sentiment score in each novel

text_sentiment_afinn_sentence %>%
  group_by(doc_id) %>%
  arrange(desc(positive_value)) %>%
  slice(1) %>%
  select(sentence_id, doc_id, sentence, positive_value, negative_value, sent_words_pos, sent_words_neg)

```

If on the other hand we are interested in looking closely at the sentiment of words within a sentence, we can 1) add a 'fake' line number to the dataset, that splits each sentence into chunks of 10 words, and print a heatmap of the sentiment of words of a specific sentence, distributing words in the x axis and the chunks in the y axis.

We can isolate one sentence and plot the sentiment of the words in the sentence. Let's first look  for a sentence that is has a high number of negative and positive words.

```{r}

# Select a the sentence that has the highest number of positive words in the text data:

text_sentiment_afinn_sentence %>%
  filter(!is.na(positive_value)) %>%
  arrange(desc(positive_value)) %>%
  slice(1) %>%
  select(sentence_id, doc_id, sentence, positive_value, negative_value, sent_words_pos, sent_words_neg)

```

Ok, now we have a sentence_id: 3955


```{r}

# Create a heatmap of the sentiment of the sentence 12540

test_sentence <- text_sentiment_afinn %>%
  filter(sentence_id == 3955, doc_id == "Mansfield Park")

test_sentence %>%
  mutate(line_number = rep(1:ceiling(n() / 10), each = 10, length.out = n())) %>%
  mutate(token_position = rep(1:10, length.out = n())) %>%
  ggplot(aes(x = token_position,
             y = -line_number,
             fill = value)) +
  geom_tile() +
  geom_text(aes(label = token), size = 3) +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none") +
  labs(title = "Sentiment of the words in the sentence 8421 (jockers lexicon)",
       subtitle = test_sentence$doc_id[1],
       x = "Word position",
       y = "Line number")


```


Of course we can do the same with a different lexicon or a different sentence.

```{r}

# Create a heatmap of the sentiment of the sentence 8374 using the jockers lexicon. We can add a subtitle using the doc_id so that we can see which text the sentence belongs to.

test_sentence <- text_sentiment_jockers %>%
  filter(sentence_id == 3955, doc_id == "Mansfield Park")

test_sentence %>%
  mutate(line_number = rep(1:ceiling(n() / 10), each = 10, length.out = n())) %>%
  mutate(token_position = rep(1:10, length.out = n())) %>%
  ggplot(aes(x = token_position,
             y = -line_number,
             fill = jockers_polarity)) +
  geom_tile() +
  geom_text(aes(label = token), size = 3) +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none") +
  labs(title = "Sentiment of the words in the sentence 8421 (jockers lexicon)",
       subtitle = test_sentence$doc_id[1],
       x = "Word position",
       y = "Line number")


```


# Visualizing the sentiment arc of texts

Another classic way to visualize the sentiment of the sentences in each novel is to create a line plot of the sentiment scores of the sentences in each text that shows the narrative arc of the sentiment of the text. We can use the ggplot2 package to create the line plot, and the geom_smooth() function to add a smoothed line to the plot.

```{r}

# Create a line plot of the sentiment scores of the sentences in each text that shows the narrative arc of the sentiment of the text

text_sentiment_jockers %>%
  filter(!is.na(jockers_polarity)) %>%
  group_by(doc_id, sentence_id) %>%
  summarise(jockers_polarity = mean(jockers_polarity, na.rm = T)) %>%
  ggplot(aes(x = sentence_id,
             y = jockers_polarity,
             group = doc_id,
             color = doc_id)) +
  geom_smooth(method = "loess") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle=60, hjust=1),
    legend.position = "none") +
  labs(title = "Sentiment arc of texts (jockers lexicon)",
       x = "Sentence",
       y = "Mean polarity score") +
  facet_wrap(~doc_id, scales = "free_x")



```



## EXCERCISE 6

Can you create a line plot of the sentiment scores of the sentences in each novel that shows the narrative arc of the sentiment of the text?

```{r}
# your turn!

```

## EXCERCISE 7

Can you compare the sentiment arcs of the novels using the bing and nrc lexicons?

```{r}
# your turn!

```


And that's it for today!

We have learned how to perform sentiment analysis on a corpus of texts using the tidytext package. We have learned how to load sentiment lexicons, join them to text data, and calculate the sentiment of a text. We have also learned how to visualize the sentiment of a text, and to summarize the sentiment of a text. We have also learned how to create a map of the sentiment of the tweets, and to create a time series of the polarity scores of the tweets. We have also learned how to create a line plot of the sentiment scores of the sentences in each text that shows the narrative arc of the sentiment of the text.

# CONCLUSION
 
Now it's time to look at your text: open the script 'SA_my_corpus.Rmd' and start working on your own corpus. You can use the same functions we used in this document to perform sentiment analysis on your own corpus. You can also use the same functions to visualize the sentiment of your own corpus. You can also use the same functions to create a map of the sentiment of your own corpus, and to create a time series of the polarity scores of your own corpus. You can also use the same functions to create a line plot of the sentiment scores of the sentences in your own corpus that shows the narrative arc of the sentiment of your own corpus.

This is the time to try and apply what you have learned to your own research. If you have any questions, feel free to ask!


# References

- https://www.tidytextmining.com/
- https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
- https://cran.r-project.org/web/packages/tidytext/tidytext.pdf
- https://www.tidytextmining.com/sentiment.html
- https://www.tidytextmining.com/sentiment.html#sentiment-lexicons